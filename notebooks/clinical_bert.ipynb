{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore a Named Entity Recognition task using transformers. The task will involve finetuning the [ClinicalBert](https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.18.5\n",
      "  Downloading numpy-1.23.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/vscode/.local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.5/503.5 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.23.0 pandas-1.4.3 pytz-2022.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.3/362.3 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-8.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.19.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/vscode/.local/lib/python3.9/site-packages (from datasets) (1.23.0)\n",
      "Requirement already satisfied: packaging in /home/vscode/.local/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/vscode/.local/lib/python3.9/site-packages (from datasets) (1.4.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.8/661.8 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/vscode/.local/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.2/160.2 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting attrs>=17.3.0\n",
      "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.2/156.2 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /home/vscode/.local/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/vscode/.local/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, urllib3, typing-extensions, tqdm, pyyaml, pyarrow, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, multiprocess, aiosignal, responses, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 attrs-21.4.0 certifi-2022.6.15 charset-normalizer-2.1.0 datasets-2.3.2 dill-0.3.5.1 filelock-3.7.1 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.8.1 idna-3.3 multidict-6.0.2 multiprocess-0.70.13 pyarrow-8.0.0 pyyaml-6.0 requests-2.28.1 responses-0.18.0 tqdm-4.64.0 typing-extensions-4.3.0 urllib3-1.26.9 xxhash-3.0.0 yarl-1.7.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.6.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.2/763.2 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/vscode/.local/lib/python3.9/site-packages (from transformers) (1.23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/vscode/.local/lib/python3.9/site-packages (from transformers) (0.8.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/vscode/.local/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vscode/.local/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.9/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: requests in /home/vscode/.local/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vscode/.local/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vscode/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/vscode/.local/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/vscode/.local/lib/python3.9/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vscode/.local/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.9/site-packages (from requests->transformers) (2022.6.15)\n",
      "Installing collected packages: tokenizers, regex, transformers\n",
      "Successfully installed regex-2022.6.2 tokenizers-0.12.1 transformers-4.20.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-1.12.0-cp39-cp39-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/vscode/.local/lib/python3.9/site-packages (from torch) (4.3.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.12.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install datasets\n",
    "! pip install transformers\n",
    "! pip install torch\n",
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Put the code below into a function later\n",
    "\n",
    "# Test Data Frame\n",
    "with open(\"../data/Bioinformatics_16/BioNLP13CG-IOB/test_tokens.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "tokens = [tokens.replace(\"\\n\", \"\").rstrip().split() for tokens in lines]\n",
    "with open(\"../data/Bioinformatics_16/BioNLP13CG-IOB/test_labels.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "labels = [tokens.replace(\"\\n\", \"\").rstrip().split() for tokens in lines]\n",
    "test = pd.DataFrame({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "\n",
    "# Validation Data Frame\n",
    "with open(\"../data/Bioinformatics_16/BioNLP13CG-IOB/validation_tokens.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "tokens = [tokens.replace(\"\\n\", \"\").rstrip().split() for tokens in lines]\n",
    "with open(\"../data/Bioinformatics_16/BioNLP13CG-IOB/validation_labels.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "labels = [tokens.replace(\"\\n\", \"\").rstrip().split() for tokens in lines]\n",
    "validation = pd.DataFrame({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "\n",
    "\n",
    "# Train Data Frame\n",
    "with open(\"../data/Bioinformatics_16/BioNLP13CG-IOB/train_tokens.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "tokens = [tokens.replace(\"\\n\", \"\").rstrip().split() for tokens in lines]\n",
    "with open(\"../data/Bioinformatics_16/BioNLP13CG-IOB/train_labels.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "labels = [tokens.replace(\"\\n\", \"\").rstrip().split() for tokens in lines]\n",
    "train = pd.DataFrame({\"tokens\": tokens, \"ner_tags\": labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[The, TGF, -, beta, type, II, receptor, in, ch...</td>\n",
       "      <td>[O, B-Gene_or_gene_product, I-Gene_or_gene_pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Genomic, instability, is, one, mechanism, pro...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [The, TGF, -, beta, type, II, receptor, in, ch...   \n",
       "1  [Genomic, instability, is, one, mechanism, pro...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [O, B-Gene_or_gene_product, I-Gene_or_gene_pro...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to a datasets dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 3021\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 1895\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_df = datasets.DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train),\n",
    "    \"validation\": Dataset.from_pandas(validation),\n",
    "    \"test\": Dataset.from_pandas(test)\n",
    "})\n",
    "\n",
    "med_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rats</td>\n",
       "      <td>B-Organism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>were</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>divided</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>into</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>groups</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tokens    ner_tags\n",
       "0      The           O\n",
       "1     rats  B-Organism\n",
       "2     were           O\n",
       "3  divided           O\n",
       "4     into           O\n",
       "5        4           O\n",
       "6   groups           O\n",
       "7        .           O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "example = pd.DataFrame(med_df[\"validation\"][221])\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record is annotated in the `inside-outside-beginning` format i.e a `B-` prefix indicates the beginning of an entity, and consecutive\n",
    "tokens belonging to the same entity are given an `I-` prefix. An `O` tag indicates that the\n",
    "token does not belong to any entity. For example, the following sentence:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick check that we don't have any unusual imbalance in the tags, let's calculate the frequencies of each entity across each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gene_or_gene_product</th>\n",
       "      <td>4016</td>\n",
       "      <td>1354</td>\n",
       "      <td>2513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cancer</th>\n",
       "      <td>1226</td>\n",
       "      <td>430</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cell</th>\n",
       "      <td>1934</td>\n",
       "      <td>540</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Organism</th>\n",
       "      <td>896</td>\n",
       "      <td>295</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Simple_chemical</th>\n",
       "      <td>1096</td>\n",
       "      <td>443</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi</th>\n",
       "      <td>415</td>\n",
       "      <td>138</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Organ</th>\n",
       "      <td>194</td>\n",
       "      <td>71</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Organism_subdivision</th>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tissue</th>\n",
       "      <td>314</td>\n",
       "      <td>86</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Immaterial_anatomical_entity</th>\n",
       "      <td>52</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Organism_substance</th>\n",
       "      <td>144</td>\n",
       "      <td>36</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cellular_component</th>\n",
       "      <td>291</td>\n",
       "      <td>95</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pathological_formation</th>\n",
       "      <td>96</td>\n",
       "      <td>44</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino_acid</th>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anatomical_system</th>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developing_anatomical_structure</th>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 train  validation  test\n",
       "Gene_or_gene_product              4016        1354  2513\n",
       "Cancer                            1226         430   918\n",
       "Cell                              1934         540   996\n",
       "Organism                           896         295   513\n",
       "Simple_chemical                   1096         443   720\n",
       "Multi                              415         138   303\n",
       "Organ                              194          71   156\n",
       "Organism_subdivision                47          12    39\n",
       "Tissue                             314          86   184\n",
       "Immaterial_anatomical_entity        52          19    31\n",
       "Organism_substance                 144          36   101\n",
       "Cellular_component                 291          95   176\n",
       "Pathological_formation              96          44    88\n",
       "Amino_acid                          40          33    62\n",
       "Anatomical_system                   21           3    17\n",
       "Developing_anatomical_structure     13           5    17"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subclass for counting hashable objects\n",
    "from collections import Counter\n",
    "# calls a factory function to supply missing values\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in med_df.items():\n",
    "    for row in dataset[\"ner_tags\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "\n",
    "\n",
    "pd.DataFrame(split2freqs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some entitites that are too few across the data sets, perhaps combine the train and validation sets? Let's see how the model will generalize on these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need a way to encode the `ner_tags` eg `Amino_acid` into a numerical form. Let's create the tags that will be used to label each entity and the mapping of each tag to an ID and vice versa. All of this information can be derived as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-Gene_or_gene_product',\n",
       " 'I-Gene_or_gene_product',\n",
       " 'B-Cancer',\n",
       " 'I-Cancer',\n",
       " 'B-Cell',\n",
       " 'I-Cell',\n",
       " 'B-Organism',\n",
       " 'B-Simple_chemical',\n",
       " 'I-Simple_chemical',\n",
       " 'B-Multi-tissue_structure',\n",
       " 'I-Multi-tissue_structure',\n",
       " 'B-Organ',\n",
       " 'B-Organism_subdivision',\n",
       " 'B-Tissue',\n",
       " 'I-Tissue',\n",
       " 'B-Immaterial_anatomical_entity',\n",
       " 'B-Organism_substance',\n",
       " 'I-Organism_substance',\n",
       " 'I-Organism',\n",
       " 'I-Organism_subdivision',\n",
       " 'B-Cellular_component',\n",
       " 'I-Immaterial_anatomical_entity',\n",
       " 'I-Cellular_component',\n",
       " 'B-Pathological_formation',\n",
       " 'I-Pathological_formation',\n",
       " 'I-Organ',\n",
       " 'B-Amino_acid',\n",
       " 'I-Amino_acid',\n",
       " 'B-Anatomical_system',\n",
       " 'I-Anatomical_system',\n",
       " 'B-Developing_anatomical_structure',\n",
       " 'I-Developing_anatomical_structure']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split2freqs = defaultdict(Counter)\n",
    "\n",
    "for split, dataset in med_df.items():\n",
    "    for row in dataset[\"ner_tags\"]:\n",
    "        for tag in row:\n",
    "            tag_type = tag\n",
    "            split2freqs[split][tag_type] +=1\n",
    "\n",
    "\n",
    "tag_names = pd.DataFrame(split2freqs).reset_index()[\"index\"].to_list()\n",
    "tag_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating `tags to index` and `index to tag` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-Developing_anatomical_structure\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Create index and tag mappings\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tag_names)}\n",
    "index2tag = {idx: tag for idx, tag in enumerate(tag_names)}\n",
    "print(index2tag[32])\n",
    "print(tag2index[\"I-Developing_anatomical_structure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these, the next step is to create a new column in each split with the numeric class label for each observation. We'll use the `map ()` method to apply a function to each observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function create_tag_ids at 0x7fb6c1ce7f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6080fedcad049bc9b67a1f78cbf93b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3021 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75c75974831450b9f74a90214882882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fb83d3cbf249679f45a8776f77ccba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1895 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'tag_ids'],\n",
       "        num_rows: 3021\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'tag_ids'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'tag_ids'],\n",
       "        num_rows: 1895\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add ner_tag ids\n",
    "def create_tag_ids(batch):\n",
    "    return {\"tag_ids\": [tag2index[ner_tag] for ner_tag in batch[\"ner_tags\"]]}\n",
    "\n",
    "# Apply function to multiple batches\n",
    "med_df = med_df.map(create_tag_ids)\n",
    "med_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>tag_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Postoperative</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>progression</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pulmonary</td>\n",
       "      <td>B-Organ</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metastasis</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>osteosarcoma</td>\n",
       "      <td>B-Cancer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokens  ner_tags  tag_ids\n",
       "0  Postoperative         O        0\n",
       "1    progression         O        0\n",
       "2             of         O        0\n",
       "3      pulmonary   B-Organ       12\n",
       "4     metastasis         O        0\n",
       "5             in         O        0\n",
       "6   osteosarcoma  B-Cancer        3\n",
       "7              .         O        0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = pd.DataFrame(med_df[\"validation\"][111])\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! We'll still need to tokenize the tokens into numeric representations. We'll get back to that in just a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main concept that makes Transformers so versatile is the split of the architecture into a body and head. This separation of bodies and heads allows us to build a custom head\n",
    "for any task and just mount it on top of a pretrained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "model_ckpt = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "AutoConfig.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the index and tag mappings and the number of distinct classes in the AutoConfig object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-Gene_or_gene_product\",\n",
       "    \"2\": \"I-Gene_or_gene_product\",\n",
       "    \"3\": \"B-Cancer\",\n",
       "    \"4\": \"I-Cancer\",\n",
       "    \"5\": \"B-Cell\",\n",
       "    \"6\": \"I-Cell\",\n",
       "    \"7\": \"B-Organism\",\n",
       "    \"8\": \"B-Simple_chemical\",\n",
       "    \"9\": \"I-Simple_chemical\",\n",
       "    \"10\": \"B-Multi-tissue_structure\",\n",
       "    \"11\": \"I-Multi-tissue_structure\",\n",
       "    \"12\": \"B-Organ\",\n",
       "    \"13\": \"B-Organism_subdivision\",\n",
       "    \"14\": \"B-Tissue\",\n",
       "    \"15\": \"I-Tissue\",\n",
       "    \"16\": \"B-Immaterial_anatomical_entity\",\n",
       "    \"17\": \"B-Organism_substance\",\n",
       "    \"18\": \"I-Organism_substance\",\n",
       "    \"19\": \"I-Organism\",\n",
       "    \"20\": \"I-Organism_subdivision\",\n",
       "    \"21\": \"B-Cellular_component\",\n",
       "    \"22\": \"I-Immaterial_anatomical_entity\",\n",
       "    \"23\": \"I-Cellular_component\",\n",
       "    \"24\": \"B-Pathological_formation\",\n",
       "    \"25\": \"I-Pathological_formation\",\n",
       "    \"26\": \"I-Organ\",\n",
       "    \"27\": \"B-Amino_acid\",\n",
       "    \"28\": \"I-Amino_acid\",\n",
       "    \"29\": \"B-Anatomical_system\",\n",
       "    \"30\": \"I-Anatomical_system\",\n",
       "    \"31\": \"B-Developing_anatomical_structure\",\n",
       "    \"32\": \"I-Developing_anatomical_structure\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"B-Amino_acid\": 27,\n",
       "    \"B-Anatomical_system\": 29,\n",
       "    \"B-Cancer\": 3,\n",
       "    \"B-Cell\": 5,\n",
       "    \"B-Cellular_component\": 21,\n",
       "    \"B-Developing_anatomical_structure\": 31,\n",
       "    \"B-Gene_or_gene_product\": 1,\n",
       "    \"B-Immaterial_anatomical_entity\": 16,\n",
       "    \"B-Multi-tissue_structure\": 10,\n",
       "    \"B-Organ\": 12,\n",
       "    \"B-Organism\": 7,\n",
       "    \"B-Organism_subdivision\": 13,\n",
       "    \"B-Organism_substance\": 17,\n",
       "    \"B-Pathological_formation\": 24,\n",
       "    \"B-Simple_chemical\": 8,\n",
       "    \"B-Tissue\": 14,\n",
       "    \"I-Amino_acid\": 28,\n",
       "    \"I-Anatomical_system\": 30,\n",
       "    \"I-Cancer\": 4,\n",
       "    \"I-Cell\": 6,\n",
       "    \"I-Cellular_component\": 23,\n",
       "    \"I-Developing_anatomical_structure\": 32,\n",
       "    \"I-Gene_or_gene_product\": 2,\n",
       "    \"I-Immaterial_anatomical_entity\": 22,\n",
       "    \"I-Multi-tissue_structure\": 11,\n",
       "    \"I-Organ\": 26,\n",
       "    \"I-Organism\": 19,\n",
       "    \"I-Organism_subdivision\": 20,\n",
       "    \"I-Organism_substance\": 18,\n",
       "    \"I-Pathological_formation\": 25,\n",
       "    \"I-Simple_chemical\": 9,\n",
       "    \"I-Tissue\": 15,\n",
       "    \"O\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_bert_config = AutoConfig.from_pretrained(model_ckpt, num_labels = len(tag_names),\n",
    "id2label = index2tag, label2id = tag2index)\n",
    "\n",
    "clinical_bert_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoConfig class contains the the blueprint of a model's architecture and is usually downloaded automatically when we run `AutoModelForTokenClassification.from_pretrained`. In this case we load the model first with the additional `config` argument of the configuration file we modified above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=33, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_ckpt, config = clinical_bert_config).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the model's tokenizer which does the task of breaking down a string into numerical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='emilyalsentzer/Bio_ClinicalBERT', vocab_size=28996, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check to ensure the model and tokenizer have been initialized correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Postoperative progression of pulmonary metastasis in osteosarcoma .'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = med_df[\"validation\"][111][\"tokens\"]\n",
    "text = \" \".join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>post</td>\n",
       "      <td>##oper</td>\n",
       "      <td>##ative</td>\n",
       "      <td>progression</td>\n",
       "      <td>of</td>\n",
       "      <td>pulmonary</td>\n",
       "      <td>meta</td>\n",
       "      <td>##sta</td>\n",
       "      <td>##sis</td>\n",
       "      <td>in</td>\n",
       "      <td>o</td>\n",
       "      <td>##ste</td>\n",
       "      <td>##osa</td>\n",
       "      <td>##rc</td>\n",
       "      <td>##oma</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>101</td>\n",
       "      <td>2112</td>\n",
       "      <td>19807</td>\n",
       "      <td>5838</td>\n",
       "      <td>16147</td>\n",
       "      <td>1104</td>\n",
       "      <td>26600</td>\n",
       "      <td>27154</td>\n",
       "      <td>8419</td>\n",
       "      <td>4863</td>\n",
       "      <td>1107</td>\n",
       "      <td>184</td>\n",
       "      <td>13894</td>\n",
       "      <td>9275</td>\n",
       "      <td>19878</td>\n",
       "      <td>7903</td>\n",
       "      <td>119</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1       2        3            4     5          6      7   \\\n",
       "Tokens     [CLS]  post  ##oper  ##ative  progression    of  pulmonary   meta   \n",
       "Input IDs    101  2112   19807     5838        16147  1104      26600  27154   \n",
       "\n",
       "              8      9     10   11     12     13     14     15   16     17  \n",
       "Tokens     ##sta  ##sis    in    o  ##ste  ##osa   ##rc  ##oma    .  [SEP]  \n",
       "Input IDs   8419   4863  1107  184  13894   9275  19878   7903  119    102  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "pd.DataFrame([tokens.tokens(), tokens[\"input_ids\"][0].numpy()], index = [\"Tokens\", \"Input IDs\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to pass the inputs to the model and extract the predictions by taking\n",
    "the argmax to get the most likely class per token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 33])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(tokens[\"input_ids\"].to(device)).logits\n",
    "\n",
    "outputs.shape #[batch_size, num_tokens, num_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.7144e-03, -2.0809e-01,  1.5450e-01, -3.1757e-01,  4.9284e-01,\n",
       "          -4.1935e-01,  6.5681e-01,  2.3797e-01,  3.3045e-01, -6.2176e-02,\n",
       "          -5.7030e-02,  1.2529e-01,  1.8328e-01, -2.7830e-01, -8.2230e-02,\n",
       "          -7.6924e-01, -4.0761e-01,  2.8337e-01, -2.7666e-01, -2.1317e-01,\n",
       "          -2.9887e-01, -1.0127e-01,  4.6710e-01,  2.2910e-01, -1.2638e-01,\n",
       "          -1.8743e-01,  1.2102e-01,  3.4941e-01, -2.9401e-01, -6.1882e-01,\n",
       "           1.5627e-01, -3.8364e-01,  7.7284e-02],\n",
       "         [-2.2685e-01, -1.6611e-01, -3.2630e-01, -5.4885e-01,  3.7400e-01,\n",
       "          -2.5464e-02,  2.4613e-01,  1.0268e-01, -3.2200e-01,  2.5381e-01,\n",
       "          -2.2599e-01,  1.8929e-01,  2.8862e-01, -4.4234e-01, -3.7602e-01,\n",
       "          -8.5255e-01, -3.4745e-02,  4.9657e-01, -4.0993e-01, -2.6837e-01,\n",
       "          -4.2423e-01, -2.2308e-01,  4.1031e-01,  4.8277e-01,  2.0932e-01,\n",
       "           1.0835e-01,  2.9885e-01,  4.7215e-01, -1.7901e-01, -2.0392e-01,\n",
       "          -6.2709e-02, -1.9199e-01,  4.9676e-03],\n",
       "         [-3.0295e-01, -7.3026e-02,  1.3377e-01, -7.6706e-02,  2.3343e-01,\n",
       "          -2.7196e-01,  7.5413e-02, -2.1862e-01, -2.8574e-02,  1.9277e-01,\n",
       "          -1.9799e-01,  4.9622e-01,  5.5586e-01, -5.2715e-01, -3.1343e-01,\n",
       "          -6.5024e-01, -3.8595e-01,  7.5667e-01,  1.2097e-03, -6.5853e-03,\n",
       "          -4.0786e-01, -1.3582e-01,  3.5615e-01,  1.2385e-01,  1.4412e-01,\n",
       "           3.7569e-02,  1.2235e-01,  6.8701e-01, -2.3023e-01, -2.6494e-01,\n",
       "          -3.5641e-01,  6.0283e-03,  1.4058e-01],\n",
       "         [-2.1538e-01, -6.0327e-02, -2.1938e-01, -1.0965e-01,  4.6571e-01,\n",
       "          -2.4475e-01,  3.1697e-01,  8.0166e-02,  1.9985e-02,  1.2551e-01,\n",
       "          -3.4109e-01,  2.2637e-01,  2.6956e-01, -5.8074e-01, -2.5587e-01,\n",
       "          -7.1361e-01, -4.7098e-01,  6.2002e-02, -2.3921e-01, -1.3288e-01,\n",
       "          -2.3771e-01, -2.5494e-01,  2.1609e-01,  2.2806e-01,  1.4324e-01,\n",
       "           9.4912e-02,  2.3294e-01,  4.2497e-01, -2.4219e-01, -4.7442e-01,\n",
       "          -1.8784e-01, -1.0706e-01, -2.0036e-02],\n",
       "         [-6.2765e-02, -1.3542e-01,  5.9664e-02, -1.7389e-01,  2.8407e-01,\n",
       "          -1.1206e-01,  5.1033e-01,  1.1461e-01,  1.6545e-02,  2.5418e-01,\n",
       "           3.4983e-02, -8.7084e-02,  3.4593e-01, -4.1770e-01, -3.1804e-01,\n",
       "          -4.8088e-01, -4.6073e-01,  1.3365e-01, -2.1220e-01, -2.1982e-01,\n",
       "          -3.8788e-01, -2.7846e-01,  1.3670e-01,  1.4841e-01,  1.3498e-01,\n",
       "           3.5250e-02,  3.6784e-01,  5.6035e-01, -2.6954e-01, -1.9784e-01,\n",
       "          -3.4657e-02, -3.7571e-01,  3.0337e-01],\n",
       "         [-7.1840e-02, -1.8626e-01, -6.0330e-02, -4.4556e-01,  2.1784e-01,\n",
       "          -2.1698e-01,  3.9421e-01,  2.1907e-01, -3.0136e-02, -3.4614e-02,\n",
       "           1.4020e-01,  2.9102e-01,  2.0951e-01, -3.6075e-01, -2.1229e-01,\n",
       "          -7.3090e-01, -3.5314e-01,  3.9676e-02, -1.2074e-01, -1.5233e-01,\n",
       "          -3.8281e-01,  8.1121e-02,  3.3413e-01,  3.9374e-01,  1.3521e-01,\n",
       "          -1.9200e-01,  3.0055e-01,  1.9328e-01, -1.7684e-01, -6.9905e-01,\n",
       "          -5.5942e-02, -3.5293e-01, -3.0746e-02],\n",
       "         [-1.5639e-01, -1.2810e-01, -2.5871e-03, -4.0175e-01,  2.7088e-01,\n",
       "          -2.3733e-01,  4.3294e-01, -8.4595e-02, -1.3688e-01,  1.6271e-02,\n",
       "          -8.1597e-02,  1.4369e-01,  4.2788e-01,  7.5230e-03, -4.7652e-01,\n",
       "          -8.3248e-01, -1.6454e-01,  2.6522e-01, -3.6159e-01, -8.3178e-02,\n",
       "          -2.4761e-01,  4.8455e-04,  2.1534e-01,  1.4178e-01, -5.5089e-02,\n",
       "           7.7740e-02,  2.4949e-01,  3.7734e-01, -2.2753e-01, -4.4790e-01,\n",
       "           1.3188e-01, -5.1002e-01, -1.6535e-02],\n",
       "         [-2.2833e-01, -1.2601e-01, -1.0763e-01, -5.3615e-03,  1.8655e-01,\n",
       "          -3.0214e-01,  4.9879e-01,  1.2275e-02, -1.8295e-01, -1.6713e-01,\n",
       "          -8.0450e-02,  9.0815e-02,  3.6700e-01,  1.0814e-01, -3.2441e-01,\n",
       "          -3.9399e-01, -2.9226e-01,  5.8704e-01, -4.8804e-01, -2.9755e-02,\n",
       "          -1.5760e-01, -3.2298e-01,  4.8854e-01,  3.0746e-01,  2.0977e-01,\n",
       "           2.0229e-01,  3.7524e-01,  5.1342e-01, -1.2738e-02,  4.6047e-02,\n",
       "           1.5198e-01, -2.1969e-01,  7.2448e-02],\n",
       "         [-1.6855e-01, -3.9172e-01, -2.2925e-01, -1.5176e-01, -2.3397e-03,\n",
       "          -3.9145e-01,  8.2910e-01, -2.2246e-02,  1.1817e-01, -7.9116e-02,\n",
       "           1.5482e-01,  3.1151e-01,  3.1834e-01, -4.6081e-02, -3.4496e-01,\n",
       "          -5.8750e-01, -1.0292e-01,  5.5370e-01, -3.3669e-01, -5.6922e-02,\n",
       "          -7.7208e-02, -3.5971e-02,  2.9326e-01,  3.0953e-01,  2.6103e-01,\n",
       "           1.6342e-01,  3.4083e-01,  3.7014e-01, -8.4104e-02,  9.2256e-02,\n",
       "           2.7458e-02, -2.5208e-01, -2.7541e-03],\n",
       "         [-2.3710e-01, -5.6067e-01,  2.9365e-02, -4.3175e-01,  1.8749e-01,\n",
       "          -3.2315e-01,  5.6584e-01, -1.1792e-01, -1.2525e-01,  2.9777e-02,\n",
       "           1.0756e-01,  5.7723e-01,  1.8095e-01, -3.3291e-02, -2.4582e-01,\n",
       "          -7.3169e-01, -7.7050e-02,  2.4532e-01, -1.3996e-01,  1.0999e-01,\n",
       "          -2.6312e-01, -1.1719e-01,  3.9925e-01,  4.5938e-01,  2.6813e-01,\n",
       "          -1.4569e-01,  2.4649e-01,  7.9226e-02, -2.6171e-02, -4.0523e-02,\n",
       "          -1.1691e-01, -1.9865e-01, -2.3166e-02],\n",
       "         [ 2.1122e-01, -2.4847e-01,  1.2430e-01, -5.3621e-01,  4.4787e-01,\n",
       "          -2.4183e-01,  6.3651e-01,  7.8308e-02, -1.7405e-01, -1.9582e-01,\n",
       "           1.4823e-01,  7.9995e-02,  5.1538e-01, -2.2556e-01,  1.8581e-02,\n",
       "          -9.3159e-01, -1.7569e-01, -1.9166e-01, -1.1223e-01, -1.8663e-01,\n",
       "          -4.0928e-01,  9.6018e-02,  1.8492e-01,  1.2180e-01,  8.8811e-02,\n",
       "          -4.7738e-02,  2.3787e-01,  3.8282e-01, -4.6443e-01, -8.4203e-01,\n",
       "          -2.4601e-01, -2.9316e-01, -8.2388e-02],\n",
       "         [-3.3097e-02, -4.6105e-02,  8.6525e-02, -2.9421e-01,  6.4936e-01,\n",
       "          -9.5264e-02,  5.4914e-01,  2.1781e-02, -6.1455e-02,  5.1375e-02,\n",
       "          -1.2630e-01,  1.5025e-01,  3.0823e-01, -8.6153e-02, -4.9705e-02,\n",
       "          -4.7226e-01, -4.9827e-01,  1.7860e-01, -3.5064e-01, -2.9928e-01,\n",
       "          -2.0475e-01, -3.1277e-02,  2.0713e-01,  2.3369e-01, -2.3439e-01,\n",
       "           2.4676e-01,  9.2515e-02,  3.0181e-01, -4.7894e-01, -2.5488e-01,\n",
       "           1.4200e-01, -2.1055e-01,  1.6339e-01],\n",
       "         [ 4.4130e-02,  1.7537e-01,  2.1701e-01, -9.4957e-02,  2.5394e-01,\n",
       "          -1.1538e-01,  4.9254e-01, -1.9279e-01,  9.3974e-03,  3.0297e-01,\n",
       "           5.4073e-02,  1.4370e-01,  5.0306e-01, -2.9323e-01, -4.6712e-02,\n",
       "          -5.0859e-01, -3.3746e-01,  2.5838e-01,  6.5158e-03,  6.9746e-02,\n",
       "          -4.6526e-01, -1.2251e-01, -6.4166e-02, -1.9181e-01, -1.8283e-01,\n",
       "           1.9410e-01,  4.3749e-01,  8.6749e-02, -5.0839e-01, -2.6195e-01,\n",
       "          -8.2827e-02,  4.4337e-02,  1.8722e-01],\n",
       "         [-1.8298e-01,  5.8136e-02, -8.3765e-02, -5.1215e-01,  4.4800e-01,\n",
       "          -3.4316e-01,  2.7087e-01,  2.4973e-01, -1.1826e-01,  3.8616e-01,\n",
       "          -1.5952e-01,  1.8328e-01,  5.0011e-01,  8.1889e-02, -2.1648e-02,\n",
       "          -5.8615e-01, -2.8535e-01,  1.5699e-01, -2.6336e-01,  2.0292e-01,\n",
       "          -3.5165e-01, -1.1016e-01,  4.4459e-01, -1.6157e-01,  2.4477e-01,\n",
       "           7.7027e-02,  1.8170e-01,  2.6743e-01, -2.3603e-01,  2.0386e-01,\n",
       "          -2.0719e-01, -9.3265e-02,  2.0729e-01],\n",
       "         [ 4.8506e-02, -1.1117e-01, -1.5788e-01, -2.8102e-01,  1.2995e-01,\n",
       "          -2.1192e-01,  6.8921e-01,  2.1888e-02, -3.2306e-02,  6.8535e-01,\n",
       "          -1.0425e-01,  1.0575e-01,  6.7759e-01, -2.0260e-01,  3.1250e-02,\n",
       "          -7.6652e-01, -8.1959e-02,  3.5471e-01, -5.3829e-01, -4.0146e-01,\n",
       "          -1.2157e-01,  3.1277e-02,  1.0542e-01, -7.3918e-02,  1.4222e-01,\n",
       "          -2.8998e-01,  9.5678e-02,  6.6105e-01, -5.4224e-01, -3.6160e-02,\n",
       "           5.4598e-02, -3.0575e-01,  3.9632e-01],\n",
       "         [-4.0116e-02, -4.3499e-01, -3.6626e-02, -3.9560e-01,  3.1712e-01,\n",
       "          -1.4149e-01,  5.2990e-01,  5.6967e-03, -2.3701e-01, -2.5325e-01,\n",
       "          -1.3559e-01,  5.7636e-01,  6.8466e-02, -4.0163e-02, -2.7255e-01,\n",
       "          -5.7770e-01, -4.3432e-01,  2.2935e-01, -1.5112e-01,  1.2843e-01,\n",
       "           6.4793e-02, -5.0696e-02,  2.9436e-01,  2.6955e-01,  1.9979e-01,\n",
       "          -2.9977e-02,  2.7760e-01,  2.7167e-01, -2.7952e-01,  1.2575e-01,\n",
       "          -3.2175e-01, -2.1005e-01,  3.8766e-02],\n",
       "         [-1.5969e-01, -3.0886e-01, -2.0150e-01, -3.8632e-01,  4.0540e-01,\n",
       "          -1.9550e-01,  5.5807e-01, -5.4909e-02, -1.0954e-01,  5.3140e-03,\n",
       "           3.7909e-02,  2.1904e-01,  1.7647e-01, -4.8840e-01, -1.5986e-01,\n",
       "          -8.3029e-01, -2.2492e-01, -1.3755e-02, -3.5546e-01, -1.2708e-01,\n",
       "          -2.4878e-01, -3.1521e-02,  5.7975e-01,  6.0597e-01,  2.0090e-01,\n",
       "          -1.3000e-01,  2.7381e-01,  4.6542e-01, -1.4072e-01, -5.9015e-01,\n",
       "           1.3094e-03, -4.4364e-01, -2.9020e-01],\n",
       "         [ 4.1613e-01, -8.0252e-02,  4.6333e-01,  9.8083e-02,  5.1746e-01,\n",
       "          -7.0555e-01,  2.0559e-01,  3.6975e-01,  1.3309e-01, -2.3235e-01,\n",
       "           4.6545e-01,  1.5737e-01,  4.9512e-01,  6.5481e-01,  1.3521e-01,\n",
       "          -6.9269e-01,  7.3935e-02,  1.7192e-01, -1.8744e-01,  2.0148e-01,\n",
       "          -5.7474e-01, -4.7764e-01,  4.6282e-01, -5.3462e-01,  1.9243e-01,\n",
       "           5.7418e-03,  3.5536e-01, -7.7741e-02, -1.0955e-01, -2.1409e-01,\n",
       "           6.6748e-01,  2.8405e-01, -2.3128e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that each token is given a logit among the 33 possible NER tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6, 17, 17,  4, 27,  6,  6, 17,  6, 11,  6,  4, 12, 12,  6, 11,\n",
       "        23, 30]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract predictions\n",
    "predictions = torch.argmax(outputs, -1).cpu().numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>post</td>\n",
       "      <td>##oper</td>\n",
       "      <td>##ative</td>\n",
       "      <td>progression</td>\n",
       "      <td>of</td>\n",
       "      <td>pulmonary</td>\n",
       "      <td>meta</td>\n",
       "      <td>##sta</td>\n",
       "      <td>##sis</td>\n",
       "      <td>in</td>\n",
       "      <td>o</td>\n",
       "      <td>##ste</td>\n",
       "      <td>##osa</td>\n",
       "      <td>##rc</td>\n",
       "      <td>##oma</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>I-Cell</td>\n",
       "      <td>B-Organism_substance</td>\n",
       "      <td>B-Organism_substance</td>\n",
       "      <td>I-Cancer</td>\n",
       "      <td>B-Amino_acid</td>\n",
       "      <td>I-Cell</td>\n",
       "      <td>I-Cell</td>\n",
       "      <td>B-Organism_substance</td>\n",
       "      <td>I-Cell</td>\n",
       "      <td>I-Multi-tissue_structure</td>\n",
       "      <td>I-Cell</td>\n",
       "      <td>I-Cancer</td>\n",
       "      <td>B-Organ</td>\n",
       "      <td>B-Organ</td>\n",
       "      <td>I-Cell</td>\n",
       "      <td>I-Multi-tissue_structure</td>\n",
       "      <td>I-Cellular_component</td>\n",
       "      <td>I-Anatomical_system</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                     1                     2         3   \\\n",
       "Tokens   [CLS]                  post                ##oper   ##ative   \n",
       "Tags    I-Cell  B-Organism_substance  B-Organism_substance  I-Cancer   \n",
       "\n",
       "                  4       5          6                     7       8   \\\n",
       "Tokens   progression      of  pulmonary                  meta   ##sta   \n",
       "Tags    B-Amino_acid  I-Cell     I-Cell  B-Organism_substance  I-Cell   \n",
       "\n",
       "                              9       10        11       12       13      14  \\\n",
       "Tokens                     ##sis      in         o    ##ste    ##osa    ##rc   \n",
       "Tags    I-Multi-tissue_structure  I-Cell  I-Cancer  B-Organ  B-Organ  I-Cell   \n",
       "\n",
       "                              15                    16                   17  \n",
       "Tokens                     ##oma                     .                [SEP]  \n",
       "Tags    I-Multi-tissue_structure  I-Cellular_component  I-Anatomical_system  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [index2tag[p] for p in predictions[0]]\n",
    "pd.DataFrame([tokens.tokens(), preds], index = [\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, our token classification layer with random weights leaves a lot to be\n",
    "desired; let’s fine-tune on some labeled data to make it better! Before doing so, let’s\n",
    "wrap the preceding steps into a helper function for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # Tokenizer text\n",
    "    tokens = tokenizer(text, return_tensors = \"pt\")\n",
    "    # Get predictions as distribution over 33 possible classes\n",
    "    outputs = model(tokens[\"input_ids\"]).logits\n",
    "    predictions = torch.argmax(outputs, -1).cpu().numpy()\n",
    "\n",
    "    # Map index to string\n",
    "    preds = [index2tag[p] for p in predictions[0]]\n",
    "    return pd.DataFrame([tokens.tokens(), preds], index = [\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>post</td>\n",
       "      <td>##oper</td>\n",
       "      <td>##ative</td>\n",
       "      <td>progression</td>\n",
       "      <td>of</td>\n",
       "      <td>pulmonary</td>\n",
       "      <td>meta</td>\n",
       "      <td>##sta</td>\n",
       "      <td>##sis</td>\n",
       "      <td>in</td>\n",
       "      <td>o</td>\n",
       "      <td>##ste</td>\n",
       "      <td>##osa</td>\n",
       "      <td>##rc</td>\n",
       "      <td>##oma</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>I-Cell</td>\n",
       "      <td>B-Organism_substance</td>\n",
       "      <td>B-Organism_substance</td>\n",
       "      <td>I-Cancer</td>\n",
       "      <td>B-Amino_acid</td>\n",
       "      <td>I-Cell</td>\n",
       "      <td>I-Cell</td>\n",
       "      <td>B-Organism_substance</td>\n",
       "      <td>I-Cell</td>\n",
       "      <td>I-Multi-tissue_structure</td>\n",
       "      <td>I-Cell</td>\n",
       "      <td>I-Cancer</td>\n",
       "      <td>B-Organ</td>\n",
       "      <td>B-Organ</td>\n",
       "      <td>I-Cell</td>\n",
       "      <td>I-Multi-tissue_structure</td>\n",
       "      <td>I-Cellular_component</td>\n",
       "      <td>I-Anatomical_system</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                     1                     2         3   \\\n",
       "Tokens   [CLS]                  post                ##oper   ##ative   \n",
       "Tags    I-Cell  B-Organism_substance  B-Organism_substance  I-Cancer   \n",
       "\n",
       "                  4       5          6                     7       8   \\\n",
       "Tokens   progression      of  pulmonary                  meta   ##sta   \n",
       "Tags    B-Amino_acid  I-Cell     I-Cell  B-Organism_substance  I-Cell   \n",
       "\n",
       "                              9       10        11       12       13      14  \\\n",
       "Tokens                     ##sis      in         o    ##ste    ##osa    ##rc   \n",
       "Tags    I-Multi-tissue_structure  I-Cell  I-Cancer  B-Organ  B-Organ  I-Cell   \n",
       "\n",
       "                              15                    16                   17  \n",
       "Tokens                     ##oma                     .                [SEP]  \n",
       "Tags    I-Multi-tissue_structure  I-Cellular_component  I-Anatomical_system  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_text(text, tags = index2tag, model = model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing texts for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While tokenizing texts for NER we need to keep in mind that the tokens are already split into words and that the tokenizer may split a word into two or more subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Postoperative', 'progression', 'of', 'pulmonary', 'metastasis', 'in', 'osteosarcoma', '.']\n",
      "[0, 0, 0, 12, 0, 0, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "example = med_df[\"validation\"][111]\n",
    "words, labels = example[\"tokens\"], example[\"tag_ids\"]\n",
    "print(words)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tokenize each word and use the `is_split_into_words` argument to tell the tokenizer that our input sequence is already split into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>post</td>\n",
       "      <td>##oper</td>\n",
       "      <td>##ative</td>\n",
       "      <td>progression</td>\n",
       "      <td>of</td>\n",
       "      <td>pulmonary</td>\n",
       "      <td>meta</td>\n",
       "      <td>##sta</td>\n",
       "      <td>##sis</td>\n",
       "      <td>in</td>\n",
       "      <td>o</td>\n",
       "      <td>##ste</td>\n",
       "      <td>##osa</td>\n",
       "      <td>##rc</td>\n",
       "      <td>##oma</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1       2        3            4   5          6     7      8   \\\n",
       "Tokens  [CLS]  post  ##oper  ##ative  progression  of  pulmonary  meta  ##sta   \n",
       "\n",
       "           9   10 11     12     13    14     15 16     17  \n",
       "Tokens  ##sis  in  o  ##ste  ##osa  ##rc  ##oma  .  [SEP]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = tokenizer(words, is_split_into_words=True)\n",
    "tokens = tokenized_input.tokens()\n",
    "pd.DataFrame([tokens], index = [\"Tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2112, 19807, 5838, 16147, 1104, 26600, 27154, 8419, 4863, 1107, 184, 13894, 9275, 19878, 7903, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the word `osteosarcoma` has been split into multiple subwords `o`, `ste`, `osa`, `rc` and `oma`. Since we are following the convention that only `osteosarcoma` should be associated with `B-cancer`, we need to mask the subword representations after the first word and the `word_ids()` function can help with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>post</td>\n",
       "      <td>##oper</td>\n",
       "      <td>##ative</td>\n",
       "      <td>progression</td>\n",
       "      <td>of</td>\n",
       "      <td>pulmonary</td>\n",
       "      <td>meta</td>\n",
       "      <td>##sta</td>\n",
       "      <td>##sis</td>\n",
       "      <td>in</td>\n",
       "      <td>o</td>\n",
       "      <td>##ste</td>\n",
       "      <td>##osa</td>\n",
       "      <td>##rc</td>\n",
       "      <td>##oma</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word_ids</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0     1       2        3            4   5          6     7   \\\n",
       "Tokens    [CLS]  post  ##oper  ##ative  progression  of  pulmonary  meta   \n",
       "Word_ids   None     0       0        0            1   2          3     4   \n",
       "\n",
       "             8      9   10 11     12     13    14     15 16     17  \n",
       "Tokens    ##sta  ##sis  in  o  ##ste  ##osa  ##rc  ##oma  .  [SEP]  \n",
       "Word_ids      4      4   5  6      6      6     6      6  7   None  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "pd.DataFrame([tokens, word_ids], index = [\"Tokens\", \"Word_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each subword has been mapped to the corresponding index. We'll use `-100` as the label for special tokens eg CLS and subwords we wish to mask during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, -100, -100, 0, 0, 12, 0, -100, -100, 0, 3, -100, -100, -100, -100, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    else:\n",
    "        label_ids.append(example[\"tag_ids\"][word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>post</td>\n",
       "      <td>##oper</td>\n",
       "      <td>##ative</td>\n",
       "      <td>progression</td>\n",
       "      <td>of</td>\n",
       "      <td>pulmonary</td>\n",
       "      <td>meta</td>\n",
       "      <td>##sta</td>\n",
       "      <td>##sis</td>\n",
       "      <td>in</td>\n",
       "      <td>o</td>\n",
       "      <td>##ste</td>\n",
       "      <td>##osa</td>\n",
       "      <td>##rc</td>\n",
       "      <td>##oma</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-Organ</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>B-Cancer</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1       2        3            4   5          6     7      8   \\\n",
       "0  [CLS]  post  ##oper  ##ative  progression  of  pulmonary  meta  ##sta   \n",
       "1   -100     0    -100     -100            0   0         12     0   -100   \n",
       "2    IGN     O     IGN      IGN            O   O    B-Organ     O    IGN   \n",
       "\n",
       "      9   10        11     12     13    14     15 16     17  \n",
       "0  ##sis  in         o  ##ste  ##osa  ##rc  ##oma  .  [SEP]  \n",
       "1   -100   0         3   -100   -100  -100   -100  0   -100  \n",
       "2    IGN   O  B-Cancer    IGN    IGN   IGN    IGN  O    IGN  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
    "\n",
    "pd.DataFrame([tokens, label_ids, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that’s it! We can clearly see how the label IDs align with the tokens, so let’s scale this out to the whole dataset by defining a single function that wraps all the logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df[\"train\"][\"tag_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "    labels = []\n",
    "\n",
    "    for idx, label in enumerate(examples[\"tag_ids\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4758a354acc943c7b04304019d086c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd2b1e6e0544dff82b5d2a9a56be6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1954df8bbecf4c5db09537cd128403c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode_med_df(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched = True, remove_columns = [\"ner_tags\", \"tag_ids\"])\n",
    "\n",
    "med_df_encoded = encode_med_df(med_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Postoperative',\n",
       "  'progression',\n",
       "  'of',\n",
       "  'pulmonary',\n",
       "  'metastasis',\n",
       "  'in',\n",
       "  'osteosarcoma',\n",
       "  '.'],\n",
       " 'input_ids': [101,\n",
       "  2112,\n",
       "  19807,\n",
       "  5838,\n",
       "  16147,\n",
       "  1104,\n",
       "  26600,\n",
       "  27154,\n",
       "  8419,\n",
       "  4863,\n",
       "  1107,\n",
       "  184,\n",
       "  13894,\n",
       "  9275,\n",
       "  19878,\n",
       "  7903,\n",
       "  119,\n",
       "  102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  12,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(med_df_encoded[\"validation\"][111])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
