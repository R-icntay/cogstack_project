---
title: "Processing IG attributions"
format: 
  html:
    number-sections: true
    toc: true
    toc-location: left
    toc-depth: 4
    code-tools: true
    code-fold: true
    code-link: true
editor: visual
execute: 
  warning: false
  message: false
bibliography: references.bib
---

This notebook outlines how entity attributions for *Cancer, Cell, Gene* and *organ* were processed.

The pre-notebook steps include:

-   Shuffling data and defining new train, validation and test sets

-   Loading a pretrained Clinical Bert model [@alsentzer2019] and the decision layer is randomly initialized.

-   Fine-tuning the model on the new data.

-   Integrated Gradients method is used to obtain entity scores

-   The process is repeated N times.

## Stability of keywords

Explanations to individual predictions are typically prone to noise due to random initializations of the decision layer and the order of training examples. Here, we try to explain an entity using a list of keywords ranked by aggregated attribution.

### Pre-processing Cancer entity attributions

In the code chunk below, attributions are preprocessed e.g restructuring output to sentences for easier handling, averaging split tokens of the same word etc.

```{r}
# Load required packages
library(tidyverse)
library(here)
options(scipen = 999)
source("cleaneR.R")

# Import data and calculate attributions of a word 
# with ref to all words in a sentence
cancer_attr <- read_bind_files(
  path = here(),
  pattern = "Cancer_attributions.csv$") %>% 
  select(-1, attr = `0`) %>% 
  mutate(sentence_id = row_number()) %>% 
  filter(str_length(attr) > 3) %>% 
  separate_rows(attr, sep = "\\}, ") %>% 
  #slice_sample(n = 10) %>% 
  mutate(row_id = row_number()) %>% 
  nest(data = attr) %>% 
  mutate(data = map(data, ~ add_scores(.x))) %>% 
  unnest(cols = data) %>% 
  mutate(tag = .$tokens %>% pluck(1) %>% str_split(pattern = "B-", n = 2) %>% unlist() %>% pluck(2))

# View some sample data
cancer_attr %>% 
  slice_head(n = 10)
```

### Extract Cancer entity Keywords

This step begins by ranking attribution words. This is done by calculating the average attribution scores per reshuffle. The top n (*100*) per reshuffle are then picked.

```{r}
# Aggregate attributions over all sentences
agg_cancer_attr <- cancer_attr %>% 
  ungroup() %>% 
  filter(attr_scores != 0) %>% 
# Calculate average attributions token scores per reshuffle 
  group_by(tokens, res_id, tag) %>% 
  summarise(
    n_occur = n(),
    mean_attr_score = mean(attr_scores)) %>% 
  ungroup() %>% group_by(res_id, tag) %>% 
# Take 100 top words per doc
  slice_max(mean_attr_score, n = 100) %>% 
  ungroup() %>%
# Find how many times a keyword occurs across across all
# resamples
  add_count(tokens, name = "n_across_resamples") %>% 
  ungroup() %>% 
  mutate(n_resamples = length(levels(res_id)),
         select_threshold = n_across_resamples/n_resamples)%>% 
  
  arrange(across(c(mean_attr_score, select_threshold), desc))


agg_cancer_attr %>% 
  slice_head(n = 10)
```

A word is considered stable if the ratio by which it is selected across the reshuffles is greater than a given percentage (say 60%).

```{r}
# Extract keywords
kw_cancer <- agg_cancer_attr %>% 
  group_by(tokens) %>% 
  filter(select_threshold > 0.5) %>% 
  ungroup()

# Keywords
kw_cancer %>% distinct(tokens)
```

### Preprocessing, aggregating and extracting keywords for Cell entity

```{r}
# Import data and calculate attributions of a word 
# with ref to all words in a sentence
cell_attr <- read_bind_files(
  path = here(),
  pattern = "Cell_attributions.csv$") %>% 
  select(-1, attr = `0`) %>% 
  mutate(sentence_id = row_number()) %>% 
  filter(str_length(attr) > 3) %>% 
  separate_rows(attr, sep = "\\}, ") %>% 
  #slice_sample(n = 10) %>% 
  mutate(row_id = row_number()) %>% 
  nest(data = attr) %>% 
  mutate(data = map(data, ~ add_scores(.x))) %>% 
  unnest(cols = data) %>% 
  mutate(tag = .$tokens %>% pluck(1) %>% str_split(pattern = "B-", n = 2) %>% unlist() %>% pluck(2))

# Aggregate attributions over all sentences
agg_cell_attr <- cell_attr %>% 
  ungroup() %>% 
  filter(attr_scores != 0) %>% 
# Calculate average attributions token scores per reshuffle 
  group_by(tokens, res_id, tag) %>% 
  summarise(
    n_occur = n(),
    mean_attr_score = mean(attr_scores)) %>% 
  ungroup() %>% group_by(res_id, tag) %>% 
# Take 100 top words per doc
  slice_max(mean_attr_score, n = 100) %>% 
  ungroup() %>%
# Find how many times a keyword occurs across across all
# resamples
  add_count(tokens, name = "n_across_resamples") %>% 
  ungroup() %>% 
  mutate(n_resamples = length(levels(res_id)),
         select_threshold = n_across_resamples/n_resamples)%>% 
  
  arrange(across(c(mean_attr_score, select_threshold), desc))


# Extract keywords
kw_cell <- agg_cell_attr %>% 
  group_by(tokens) %>% 
  filter(select_threshold > 0.5) %>% 
  ungroup()

# Keywords
kw_cell %>% distinct(tokens)
```

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAaCAYAAADFTB7LAAAAcklEQVR42u3OywmAQBAD0IAufvrvw4PHbUG87FVsQBALUEaDHYwuIiSQ42QeoCg/y1bD9gZ2tL7ylhvZgFOALRVsdZa33MgGTOF+MDvL2ySggAIKKKCAAgr4GXAoYePDciMbMF7jfQHrnOVtzAlUlJdzAjEsX7Hb176LAAAAAElFTkSuQmCC "Run Current Chunk")

### Preprocessing, aggregating and extracting keywords for Organ entity

```{r}
# Import data and calculate attributions of a word 
# with ref to all words in a sentence
organ_attr <- read_bind_files(
  path = here(),
  pattern = "Organ_attributions.csv$") %>% 
  select(-1, attr = `0`) %>% 
  mutate(sentence_id = row_number()) %>% 
  filter(str_length(attr) > 3) %>% 
  separate_rows(attr, sep = "\\}, ") %>% 
  #slice_sample(n = 10) %>% 
  mutate(row_id = row_number()) %>% 
  nest(data = attr) %>% 
  mutate(data = map(data, ~ add_scores(.x))) %>% 
  unnest(cols = data) %>% 
  mutate(tag = .$tokens %>% pluck(1) %>% str_split(pattern = "B-", n = 2) %>% unlist() %>% pluck(2))

# Aggregate attributions over all sentences
agg_organ_attr <- organ_attr %>% 
  ungroup() %>% 
  filter(attr_scores != 0) %>% 
# Calculate average attributions token scores per reshuffle 
  group_by(tokens, res_id, tag) %>% 
  summarise(
    n_occur = n(),
    mean_attr_score = mean(attr_scores)) %>% 
  ungroup() %>% group_by(res_id, tag) %>% 
# Take 100 top words per doc
  slice_max(mean_attr_score, n = 100) %>% 
  ungroup() %>%
# Find how many times a keyword occurs across across all
# resamples
  add_count(tokens, name = "n_across_resamples") %>% 
  ungroup() %>% 
  mutate(n_resamples = length(levels(res_id)),
         select_threshold = n_across_resamples/n_resamples)%>% 
  
  arrange(across(c(mean_attr_score, select_threshold), desc))


# Extract keywords
kw_organ <- agg_organ_attr %>% 
  group_by(tokens) %>% 
  filter(select_threshold > 0.5) %>% 
  ungroup()

# Keywords
kw_organ %>% distinct(tokens)
```

### Preprocessing, aggregating and extracting keywords for Gene entity

```{r}
# Import data and calculate attributions of a word 
# with ref to all words in a sentence
gene_attr <- read_bind_files(
  path = here(),
  pattern = "product_attributions.csv$") %>% 
  select(-1, attr = `0`) %>% 
  mutate(sentence_id = row_number()) %>% 
  filter(str_length(attr) > 3) %>% 
  separate_rows(attr, sep = "\\}, ") %>% 
  #slice_sample(n = 10) %>% 
  mutate(row_id = row_number()) %>% 
  nest(data = attr) %>% 
  mutate(data = map(data, ~ add_scores(.x))) %>% 
  unnest(cols = data) %>% 
  mutate(tag = .$tokens %>% pluck(1) %>% str_split(pattern = "B-", n = 2) %>% unlist() %>% pluck(2))

# Aggregate attributions over all sentences
agg_gene_attr <- gene_attr %>% 
  ungroup() %>% 
  filter(attr_scores != 0) %>% 
# Calculate average attributions token scores per reshuffle 
  group_by(tokens, res_id, tag) %>% 
  summarise(
    n_occur = n(),
    mean_attr_score = mean(attr_scores)) %>% 
  ungroup() %>% group_by(res_id, tag) %>% 
# Take 100 top words per doc
  slice_max(mean_attr_score, n = 100) %>% 
  ungroup() %>%
# Find how many times a keyword occurs across across all
# resamples
  add_count(tokens, name = "n_across_resamples") %>% 
  ungroup() %>% 
  mutate(n_resamples = length(levels(res_id)),
         select_threshold = n_across_resamples/n_resamples)%>% 
  
  arrange(across(c(mean_attr_score, select_threshold), desc))


# Extract keywords
kw_gene <- agg_gene_attr %>% 
  group_by(tokens) %>% 
  filter(select_threshold > 0.5) %>% 
  ungroup()

# Keywords
kw_gene %>% distinct(tokens)
```

## Lexical measures of usefulness

Now that we extracted the stable keywords across different reshuffles and models, we evaluate the keyword quality using the following measures:

### Distinctiveness (intrinsic)

This measure assesses the distinctiveness of keywords, by looking at the keyword overlap. It measures the fraction of keywords unique to a class, averaged across classes:

$$
Dist_{int} = \frac{1}{|C|} \sum_{c\in C} \frac{\{|k|k \in K_c \ \backslash  \ K_{\neg c} \}}{|K_c|}
$$

for the set of classes C and keywords K for class c or all other classes $\neg c$.

```{r}
# Measure distinctiveness in the classes
distinct_measure(kw_list = list(distinct(kw_cancer, tokens, .keep_all = TRUE), distinct(kw_cell, tokens, .keep_all = TRUE), distinct(kw_organ, tokens, .keep_all = TRUE), distinct(kw_gene, tokens, .keep_all = TRUE)))

```

The tibble above shows the number of `stable keywords` for an entity (words that were selected in more than 50% of the evaluation runs), the number of `disinct keywords` (keywords that are only associated with that specific entity), and the average across all classes. So the intrinsic distinctiveness of the keywords for these entities is:

```{r}
# Distinctiveness (intrinsic)
distinct_measure(kw_list = list(distinct(kw_cancer, tokens, .keep_all = TRUE), distinct(kw_cell, tokens, .keep_all = TRUE), distinct(kw_organ, tokens, .keep_all = TRUE), distinct(kw_gene, tokens, .keep_all = TRUE))) %>% 
  distinct(overall_dm)
```

### Coverage

Coverage in an entity is defined as the average proportion of keywords that occur across all its documents, and the global coverage measure as the average across all entities:

$$
Cov = \frac{1}{|C|} \sum_{c\in C} \frac{1}{|T_c|} \sum_{t\in T_c} \frac{\{|k|k \in K_c \cap t \}}{|K_c|}
$$

```{r}
# Calculate coverage of keywords in corresponding attributions datasets

# Make a list of attribution datasets
attributions_list <- list(
  cancer_attr,
  cell_attr,
  gene_attr,
  organ_attr
  
)


# Corresponding list of keywords
key_word_list <- list(
 (kw_cancer %>% distinct(tokens) %>% pull(tokens)),
  (kw_cell %>% distinct(tokens) %>% pull(tokens)),
 (kw_gene %>% distinct(tokens) %>% pull(tokens)),
  (kw_organ %>% distinct(tokens) %>% pull(tokens))
)

# Calculate the coverage of keywords for each corresponding attribution set
pmap_dfr(list(attributions_list, key_word_list), coverage_measure) %>% 
  mutate(overall_coverage = mean(coverage))
```

We can see that on average, about `3.7%` of `Cancer keywords` occur in the set of texts that had a `Cancer entity` and the global coverage average of keywords across their corresponding texts is about `4.8%`

::: callout-important
To do:

Discuss with Serge whether the above makes sense.
:::

### Distinctiveness (extrinsic)

Extrinsic measure of distinctiveness is defined as the coverage of keywords within a class relative to the coverage across the class boundary of unrelated documents.

A `cross-coverage` measure, similar to the `coverage` measure is defined as:

$$
XCov = \frac{1}{|C|} \sum_{c\in C} \frac{1}{|T_{\neg c}|} \sum_{t\in T_{\neg c}} \frac{\{|k|k \in K_c \cap t \}}{|K_c|}
$$

for keywords $K$ and texts $T_{\neg c}$ which is the set of texts not labeled with class $c$.

```{r}
ext_coverage <- function(tbl_list, tag_list, kw_list){
  
  # Combine all attributes
  all_attr = tbl_list %>%
    bind_rows()

# Find all sentences in a tag e.g cancer, organ
  all_sentences <- all_attr %>%
    distinct(sentence_id, tag)

# Find tags in the tbl list
  # tags = all_sentences %>%
  #   distinct(tag) %>% pull(tag)
 #tags = "Organ"
  
# Apply a function for each tag
  # purrr::pmap(list(list(tags), kw_list), ext_coverage_engine(all_sentences, all_attr))
  
purrr::pmap(list(tag_list, kw_list), ~ ext_coverage_engine(.x, .y, all_sentences = all_sentences, all_attr = all_attr))
  
}
```

```{r}
# Function that takes a tibble of attributions and corresponding key words
# and calculates coverage
coverage_measure <- function(attributions, key_words){
  attributions %>%
    # Remove tokens e.g SEP, CLS
    filter(attr_scores != 0) %>%
    # Account for repeating sentences
    group_by(res_id, sentence_id, row_id) %>% 
    # Find sentence tokens which are in the keywords
    filter(tokens %in% key_words) %>% 
    # Find how many they are
    mutate(kw_in_sntnc = n()) %>% 
    ungroup() %>% 
    # Calculate coverage
    mutate(total_kw = length(key_words),
           coverage = kw_in_sntnc/total_kw) %>% 
    distinct(sentence_id, coverage) %>% 
    summarise(tag = distinct(attributions, tag) %>% pull(tag), coverage = mean(coverage)) %>% 
    ungroup()
      
}


coverage_measure2 <- function(attributions, key_words, ...){
  attributions %>%
    # Remove tokens e.g SEP, CLS
    filter(attr_scores != 0) %>%
    # Account for repeating sentences
    group_by(res_id, sentence_id, row_id, ...) %>% 
    # Find sentence tokens which are in the keywords
    filter(tokens %in% key_words) %>% 
    # Find how many they are
    mutate(kw_in_sntnc = n()) %>% 
    ungroup() %>% 
    # Calculate coverage
    mutate(total_kw = length(key_words),
           coverage = kw_in_sntnc/total_kw) %>% 
    distinct(sentence_id, ..., coverage) %>% 
    summarise(coverage = mean(coverage)) %>% 
    pull(coverage)
      
}
```

```{r}
coverage_measure(attributions = cancer_attr, key_words = (kw_cancer %>% distinct(tokens) %>% pull(tokens)))
```
